affinity: {}
binaryFiles: {}
config:
  config.reload.automatic: "true"
  path.config: /usr/share/logstash/pipeline
  path.data: /usr/share/logstash/data
  queue.checkpoint.writes: 1
  queue.drain: "true"
  queue.max_bytes: 1gb
  queue.type: persisted
  ssl: "true"
elasticsearch:
  host: quickstart-es-http.default.svc.cluster.local
  port: 9200
exporter:
  logstash:
    enabled: false
    env: {}
    image:
      pullPolicy: IfNotPresent
      repository: bonniernews/logstash_exporter
      tag: v0.1.2
    livenessProbe:
      failureThreshold: 8
      httpGet:
        path: /metrics
        port: ls-exporter
      periodSeconds: 15
      successThreshold: 1
      timeoutSeconds: 60
    path: /metrics
    port: 9198
    readinessProbe:
      failureThreshold: 8
      httpGet:
        path: /metrics
        port: ls-exporter
      periodSeconds: 15
      successThreshold: 1
      timeoutSeconds: 60
    resources: {}
    target:
      path: /metrics
      port: 9600
  serviceMonitor:
    enabled: false
    interval: 10s
    labels: {}
    port: metrics
    scheme: http
    scrapeTimeout: 10s
extraEnv: []
extraInitContainers: []
files: null
filters: null
image:
  pullPolicy: IfNotPresent
  repository: docker.elastic.co/logstash/logstash-oss
  tag: 7.1.1
ingress:
  annotations: {}
  enabled: false
  hosts:
  - logstash.cluster.local
  path: /
  tls: []
inputs:
  main: |-
    input {
      # udp {
      #   port => 1514
      #   type => syslog
      # }
      # tcp {
      #   port => 1514
      #   type => syslog
      # }
      beats {
        port => 5044
      }
      # http {
      #   port => 8080
      # }
      # kafka {
      #   ## ref: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html
      #   bootstrap_servers => "kafka-input:9092"
      #   codec => json { charset => "UTF-8" }
      #   consumer_threads => 1
      #   topics => ["source"]
      #   type => "example"
      # }
    }
livenessProbe:
  httpGet:
    path: /
    port: monitor
  initialDelaySeconds: 20
logstashJavaOpts: -Xmx1g -Xms1g
nodeSelector: {}
outputs:
  main: |-
    output {
      stdout { codec => rubydebug }
      elasticsearch {
        hosts => ["https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
        user => elastic
        password => dnm6djck9tpv5f8nxq28bxvn
        cacert => "/usr/share/logstash/data/ca.crt"
        ssl => true
        manage_template => false
        index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
      }
      # kafka {
      #   ## ref: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html
      #   bootstrap_servers => "kafka-output:9092"
      #   codec => json { charset => "UTF-8" }
      #   compression_type => "lz4"
      #   topic_id => "destination"
      # }
    }
patterns: null
persistence:
  accessMode: ReadWriteOnce
  enabled: true
  size: 2Gi
podAnnotations: {}
podDisruptionBudget:
  maxUnavailable: 1
podLabels: {}
podManagementPolicy: OrderedReady
ports:
- containerPort: 5044
  name: beats
  protocol: TCP
priorityClassName: ""
readinessProbe:
  httpGet:
    path: /
    port: monitor
  initialDelaySeconds: 20
replicaCount: 1
resources: {}
securityContext:
  fsGroup: 1000
  runAsUser: 1000
service:
  annotations: {}
  ports:
    beats:
      port: 5044
      protocol: TCP
      targetPort: beats
  type: ClusterIP
serviceAccount:
  create: true
  name: null
terminationGracePeriodSeconds: 30
tolerations: []
updateStrategy:
  type: RollingUpdate
volumeMounts:
- mountPath: /usr/share/logstash/data
  name: data
- mountPath: /usr/share/logstash/patterns
  name: patterns
- mountPath: /usr/share/logstash/files
  name: files
- mountPath: /usr/share/logstash/pipeline
  name: pipeline
volumes: []
